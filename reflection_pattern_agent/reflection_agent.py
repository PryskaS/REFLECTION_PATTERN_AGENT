from openai import OpenAI
import os

class ReflectionAgent:
    """
    An agent that improves its output through a cycle of generation and reflection.
    """
    def __init__(self, model: str = "gpt-3.5-turbo"):
        """
        Initializes the agent with an OpenAI client.
        Assumes the OPENAI_API_KEY is set in the environment variables.
        """
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = model

    # The generation method
    def generate(self, prompt: str, reflection_history: list[str] = None) -> str:
        """
        Generates a response based on a prompt and an optional history of reflections.
        """
        system_prompt = "You are a helpful assistant."
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt},
        ]

        # This is how the agent "learns" from its self-critique.
        if reflection_history:
            reflection_context = "\n".join(reflection_history)
            messages.append(
                {"role": "assistant", "content": f"Please incorporate these reflections into your next response: {reflection_context}"}
            )

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"An error occurred during generation: {e}")
            return None # Or raise a custom exception
        
    # The reflection method
    def reflect(self, prompt: str, generated_output: str) -> list[str]:
        """
        Analyzes the generated output and provides constructive criticism.
        Returns a list of reflections/suggestions for improvement.
        """
        # The system prompt sets the role of the LLM in this reflection phase. We want it
        # to act as a critic of its own work, which is a powerful technique.
        system_prompt = """
        You are a meticulous editor and critic. Your task is to analyze a given prompt
        and the response it generated. Identify flaws, missed opportunities, or areas
        for improvement in the response. Provide a concise, bulleted list of actionable
        suggestions. Do not rewrite the response yourself, only provide the critique.
        """

        # Construct a user message that contains both the original request and the draft.
        reflection_prompt = f"""
        Original Prompt: "{prompt}"
        ---
        Generated Response:
        "{generated_output}"
        ---
        Please provide your critique of the response based on the original prompt.
        """

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": reflection_prompt},
        ]

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.5, # We want more deterministic, critical feedback
            )
            response_content = response.choices[0].message.content
            
            # Parse the response into a list of reflections.
            # This simple split is a good start. More robust parsing could use regex or other tools.
            reflections = [line.strip() for line in response_content.strip().split('\n') if line.strip()]
            return reflections

        except Exception as e:
            print(f"An error occurred during reflection: {e}")
            return [] # Return an empty list on failure to not break the chain.

    # The main orchestrator method
    def run(self, prompt: str) -> tuple[str, list[str], str]:
        """
        Executes the full generate-reflect-generate cycle.

        Returns:
            A tuple containing:
            - initial_draft (str): The first response generated.
            - reflections (list[str]): The critique generated by the reflect step.
            - final_output (str): The refined response after incorporating the reflections.
        """

        # Step 1: Generate the initial draft.
        print("--- Generating initial draft... ---")
        initial_draft = self.generate(prompt)
        if not initial_draft:
            print("Failed to generate initial draft. Aborting.")
            return None, [], None # Graceful exit

        # Step 2: Reflect on the initial draft.
        print("--- Reflecting on the draft... ---")
        reflections = self.reflect(prompt, initial_draft)
        if not reflections:
            print("--- No reflections were generated. Returning initial draft. ---")
            return initial_draft, [], initial_draft # Return the draft if reflection fails

        # Step 3: Generate the final, refined output using the reflections.
        print("--- Generating refined output based on reflections... ---")
        final_output = self.generate(prompt, reflection_history=reflections)
        if not final_output:
            print("Failed to generate final output. Returning initial draft.")
            return initial_draft, reflections, initial_draft # Fallback

        print("--- Cycle complete. ---")
        return initial_draft, reflections, final_output